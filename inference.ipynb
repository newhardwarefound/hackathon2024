{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from taprosoft's github: https://github.com/taprosoft/llm_finetuning/blob/efa6df245fee4faf27206d84802d8f58d4b6e77d/inference.py#L20\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaTokenizer)\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = \"{{your_huggingface_hub_token}}\"\n",
    "\n",
    "def load_hf_model(\n",
    "    base_model,\n",
    "    mode=8,\n",
    "    gradient_checkpointing=False,\n",
    "    device_map=\"auto\",\n",
    "):\n",
    "    kwargs = {\"device_map\": device_map}\n",
    "    if mode == 8:\n",
    "        kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            llm_int8_threshold=0.0,\n",
    "        )\n",
    "    elif mode == 4:\n",
    "        kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "    elif mode == 16:\n",
    "        kwargs[\"torch_dtype\"] = torch.float16\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(base_model, **kwargs)\n",
    "\n",
    "    # setup tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "\n",
    "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "    tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_hf_model(\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    mode=4,\n",
    "    gradient_checkpointing=False,\n",
    "    device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "sequences = [\"<s>[INST] <<SYS>> You are a helpful assistant. <</SYS>>\\\n",
    "Extract the place names from the given sentence. [\\INST]\\n\\\n",
    "The capital of the United States is Washington D.C.\"]\n",
    "\n",
    "inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    generation_config=GenerationConfig(\n",
    "        do_sample=True,\n",
    "        max_new_tokens=512,\n",
    "        top_p=0.99,\n",
    "        temperature=1e-8,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
